{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&P stock prices prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this norebook we are going to use simple neural network to predict the stock prices. We are going to use tensorflow to build the model. It is jut simple tutorial to get familiar with the deep learning and how it can be used to predict stock prizes. Actual prediction of stock prices is a really challenging and complex task that requires tremendous efforts, especially at higher frequencies, such as minutes used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's start by importing useful python packages. The data consisted of index as well as stock prices of the S&P’s 500 constituents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import our data. The data has 501 culumns and around 41266 rows. Out of them we will see first five to just sure that we have read data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SP500</th>\n",
       "      <th>NASDAQ.AAL</th>\n",
       "      <th>NASDAQ.AAPL</th>\n",
       "      <th>NASDAQ.ADBE</th>\n",
       "      <th>NASDAQ.ADI</th>\n",
       "      <th>NASDAQ.ADP</th>\n",
       "      <th>NASDAQ.ADSK</th>\n",
       "      <th>NASDAQ.AKAM</th>\n",
       "      <th>NASDAQ.ALXN</th>\n",
       "      <th>NASDAQ.AMAT</th>\n",
       "      <th>...</th>\n",
       "      <th>NYSE.WYN</th>\n",
       "      <th>NYSE.XEC</th>\n",
       "      <th>NYSE.XEL</th>\n",
       "      <th>NYSE.XL</th>\n",
       "      <th>NYSE.XOM</th>\n",
       "      <th>NYSE.XRX</th>\n",
       "      <th>NYSE.XYL</th>\n",
       "      <th>NYSE.YUM</th>\n",
       "      <th>NYSE.ZBH</th>\n",
       "      <th>NYSE.ZTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2363.6101</td>\n",
       "      <td>42.3300</td>\n",
       "      <td>143.6800</td>\n",
       "      <td>129.6300</td>\n",
       "      <td>82.040</td>\n",
       "      <td>102.2300</td>\n",
       "      <td>85.2200</td>\n",
       "      <td>59.760</td>\n",
       "      <td>121.52</td>\n",
       "      <td>38.99</td>\n",
       "      <td>...</td>\n",
       "      <td>84.370</td>\n",
       "      <td>119.035</td>\n",
       "      <td>44.40</td>\n",
       "      <td>39.88</td>\n",
       "      <td>82.03</td>\n",
       "      <td>7.36</td>\n",
       "      <td>50.22</td>\n",
       "      <td>63.86</td>\n",
       "      <td>122.000</td>\n",
       "      <td>53.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2364.1001</td>\n",
       "      <td>42.3600</td>\n",
       "      <td>143.7000</td>\n",
       "      <td>130.3200</td>\n",
       "      <td>82.080</td>\n",
       "      <td>102.1400</td>\n",
       "      <td>85.6500</td>\n",
       "      <td>59.840</td>\n",
       "      <td>121.48</td>\n",
       "      <td>39.01</td>\n",
       "      <td>...</td>\n",
       "      <td>84.370</td>\n",
       "      <td>119.035</td>\n",
       "      <td>44.11</td>\n",
       "      <td>39.88</td>\n",
       "      <td>82.03</td>\n",
       "      <td>7.38</td>\n",
       "      <td>50.22</td>\n",
       "      <td>63.74</td>\n",
       "      <td>121.770</td>\n",
       "      <td>53.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2362.6799</td>\n",
       "      <td>42.3100</td>\n",
       "      <td>143.6901</td>\n",
       "      <td>130.2250</td>\n",
       "      <td>82.030</td>\n",
       "      <td>102.2125</td>\n",
       "      <td>85.5100</td>\n",
       "      <td>59.795</td>\n",
       "      <td>121.93</td>\n",
       "      <td>38.91</td>\n",
       "      <td>...</td>\n",
       "      <td>84.585</td>\n",
       "      <td>119.260</td>\n",
       "      <td>44.09</td>\n",
       "      <td>39.98</td>\n",
       "      <td>82.02</td>\n",
       "      <td>7.36</td>\n",
       "      <td>50.12</td>\n",
       "      <td>63.75</td>\n",
       "      <td>121.700</td>\n",
       "      <td>53.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2364.3101</td>\n",
       "      <td>42.3700</td>\n",
       "      <td>143.6400</td>\n",
       "      <td>130.0729</td>\n",
       "      <td>82.000</td>\n",
       "      <td>102.1400</td>\n",
       "      <td>85.4872</td>\n",
       "      <td>59.620</td>\n",
       "      <td>121.44</td>\n",
       "      <td>38.84</td>\n",
       "      <td>...</td>\n",
       "      <td>84.460</td>\n",
       "      <td>119.260</td>\n",
       "      <td>44.25</td>\n",
       "      <td>39.99</td>\n",
       "      <td>82.02</td>\n",
       "      <td>7.35</td>\n",
       "      <td>50.16</td>\n",
       "      <td>63.88</td>\n",
       "      <td>121.700</td>\n",
       "      <td>53.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2364.8501</td>\n",
       "      <td>42.5378</td>\n",
       "      <td>143.6600</td>\n",
       "      <td>129.8800</td>\n",
       "      <td>82.035</td>\n",
       "      <td>102.0600</td>\n",
       "      <td>85.7001</td>\n",
       "      <td>59.620</td>\n",
       "      <td>121.60</td>\n",
       "      <td>38.93</td>\n",
       "      <td>...</td>\n",
       "      <td>84.470</td>\n",
       "      <td>119.610</td>\n",
       "      <td>44.11</td>\n",
       "      <td>39.96</td>\n",
       "      <td>82.03</td>\n",
       "      <td>7.36</td>\n",
       "      <td>50.20</td>\n",
       "      <td>63.91</td>\n",
       "      <td>121.695</td>\n",
       "      <td>53.240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SP500  NASDAQ.AAL  NASDAQ.AAPL  NASDAQ.ADBE  NASDAQ.ADI  NASDAQ.ADP  \\\n",
       "0  2363.6101     42.3300     143.6800     129.6300      82.040    102.2300   \n",
       "1  2364.1001     42.3600     143.7000     130.3200      82.080    102.1400   \n",
       "2  2362.6799     42.3100     143.6901     130.2250      82.030    102.2125   \n",
       "3  2364.3101     42.3700     143.6400     130.0729      82.000    102.1400   \n",
       "4  2364.8501     42.5378     143.6600     129.8800      82.035    102.0600   \n",
       "\n",
       "   NASDAQ.ADSK  NASDAQ.AKAM  NASDAQ.ALXN  NASDAQ.AMAT    ...     NYSE.WYN  \\\n",
       "0      85.2200       59.760       121.52        38.99    ...       84.370   \n",
       "1      85.6500       59.840       121.48        39.01    ...       84.370   \n",
       "2      85.5100       59.795       121.93        38.91    ...       84.585   \n",
       "3      85.4872       59.620       121.44        38.84    ...       84.460   \n",
       "4      85.7001       59.620       121.60        38.93    ...       84.470   \n",
       "\n",
       "   NYSE.XEC  NYSE.XEL  NYSE.XL  NYSE.XOM  NYSE.XRX  NYSE.XYL  NYSE.YUM  \\\n",
       "0   119.035     44.40    39.88     82.03      7.36     50.22     63.86   \n",
       "1   119.035     44.11    39.88     82.03      7.38     50.22     63.74   \n",
       "2   119.260     44.09    39.98     82.02      7.36     50.12     63.75   \n",
       "3   119.260     44.25    39.99     82.02      7.35     50.16     63.88   \n",
       "4   119.610     44.11    39.96     82.03      7.36     50.20     63.91   \n",
       "\n",
       "   NYSE.ZBH  NYSE.ZTS  \n",
       "0   122.000    53.350  \n",
       "1   121.770    53.350  \n",
       "2   121.700    53.365  \n",
       "3   121.700    53.380  \n",
       "4   121.695    53.240  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data_stocks.csv')\n",
    "\n",
    "# Drop date variable\n",
    "data = data.drop(['DATE'], 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert the data into numpy array so that we can do some operations on it. We also want to split the data into two part, one the training part and the other testing part. This way we can see how our model in generalizing to new data after training. Letter on it can help us in debugging the bias and variamce problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dimensions of dataset\n",
    "n = data.shape[0]\n",
    "p = data.shape[1]\n",
    "\n",
    "# Make data a numpy array\n",
    "data = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training and test data\n",
    "train_start = 0\n",
    "train_end = int(np.floor(0.8*n))\n",
    "test_start = train_end + 1\n",
    "test_end = n\n",
    "data_train = data[np.arange(train_start, train_end), :]\n",
    "data_test = data[np.arange(test_start, test_end), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we have splitted the data into two part as we have decided before. We are just using numpy to slice the array in to two different parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will normalize the data. Normalizing the data is really important as it can cause problems in training the model. For normalizing the data we are going to use Scikit-Learn MinMaxScaler function. For more information on the function please visit the official documentation of Scikit-Learn http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have normalized data. Let's separate the labels and features from training and testing parts of data.For this we are just doing simple numpy slicing of array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build X and y\n",
    "X_train = data_train[:, 1:]\n",
    "y_train = data_train[:, 0]\n",
    "X_test = data_test[:, 1:]\n",
    "y_test = data_test[:, 0]\n",
    "# Number of stocks in training data\n",
    "n_stocks = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have train and test data, it is time to build a simple neural network so that we can train it to make prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use tensorflow to build our model of neural network. Tensorflow is the most popular deep learning library for not just doing research but for production as well. For more information regarding the tensorflow please visit its official site https://www.tensorflow.org/ . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below block we are using placeholder, a tensorflow variable to store the data which is not going to change during the actual training. These are the inputs to neurons and the output of the final layer. X is our input feature vector while Y is our final output predicted by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholder\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, n_stocks])\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the architecture of our model. we will set the input vector to 500 i.e, n_stocks. The first hidden layer have 1024 neurons,second layer has 512 neurons,the third layer has 256 neurons, the fourth has 128 neurons while the output layer has only one neuron as we just wish one output from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model architecture parameters\n",
    "n_stocks = 500\n",
    "n_neurons_1 = 1024\n",
    "n_neurons_2 = 512\n",
    "n_neurons_3 = 256\n",
    "n_neurons_4 = 128\n",
    "n_target = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural network it is necessary to initialize the weights to some random numbers to solve the \"symmetry breaking problem\". So here we are going to use in-built weight initializer of tensorflow.\n",
    "\n",
    "To initialize the bais terms we just set them to zero as it is not going to affect the training of our neural network as well as it is not going to cause any symmentry breaking problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializers\n",
    "sigma = 1\n",
    "weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n",
    "bias_initializer = tf.zeros_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have initialized our weights and baises. We are going to define the size of weights and baises for each hidden layer. In tensorflow there is built-in function as tf.variable(), which will create a variable that can be\n",
    "changed  later on during the training. So we are going to use that for defining the size of our weights and baises. \n",
    "\n",
    "The wights of each layer has shape as (number of neurons in previous layer,number of neurons in this layer). \n",
    "The bais terms has shape as (number of neurons in this layer,1). \n",
    "\n",
    "The above two shape size is in genralized form. So you can apply that to any layer you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 1: Variables for hidden weights and biases\n",
    "W_hidden_1 = tf.Variable(weight_initializer([n_stocks, n_neurons_1]))\n",
    "bias_hidden_1 = tf.Variable(bias_initializer([n_neurons_1]))\n",
    "\n",
    "# Layer 2: Variables for hidden weights and biases\n",
    "W_hidden_2 = tf.Variable(weight_initializer([n_neurons_1, n_neurons_2]))\n",
    "bias_hidden_2 = tf.Variable(bias_initializer([n_neurons_2]))\n",
    "\n",
    "# Layer 3: Variables for hidden weights and biases\n",
    "W_hidden_3 = tf.Variable(weight_initializer([n_neurons_2, n_neurons_3]))\n",
    "bias_hidden_3 = tf.Variable(bias_initializer([n_neurons_3]))\n",
    "\n",
    "# Layer 4: Variables for hidden weights and biases\n",
    "W_hidden_4 = tf.Variable(weight_initializer([n_neurons_3, n_neurons_4]))\n",
    "bias_hidden_4 = tf.Variable(bias_initializer([n_neurons_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output layer: Variables for output weights and biases\n",
    "W_out = tf.Variable(weight_initializer([n_neurons_4, n_target]))\n",
    "bias_out = tf.Variable(bias_initializer([n_target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to perform the real calculation for neural network. \n",
    "\n",
    "The calculation for each neuron can be devide in to two parts. 1:- Linear calculation 2.:- Activation function \n",
    "\n",
    "For the first part each neuron will compute Z=(W*X)+b , W is weights,X is features and b is bais term for that neuron or the layer.\n",
    "\n",
    "For the second part of calculation we are going to feed the Z calculated above to some activation function. There are lot of activation functions available but we are going to use ReLU,which is regaded as standard activation.\n",
    "\n",
    "We can perform above two step calculation ,not just for each neuron but as well as for each layer , by just using single line of tensorflow code as below. we will do that for each hidden layter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hidden layer\n",
    "hidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))\n",
    "hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))\n",
    "hidden_3 = tf.nn.relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))\n",
    "hidden_4 = tf.nn.relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the output layer we simply add the actication computed by the previous layer. As it is regression problem we are not going to use any activation for the output layer. \n",
    "\n",
    "We can do it by just single line of code as written below in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output layer (must be transposed)\n",
    "out = tf.transpose(tf.add(tf.matmul(hidden_4, W_out), bias_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set our cost function. We are going to use Roor Mean Square error for this as it is regression problem,later on we will minimize this function by using as optimization algorithm. \n",
    "\n",
    "In tensorflow we can define cost function in just single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost function\n",
    "mse = tf.reduce_mean(tf.squared_difference(out, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use our optimizer algorithm to minimize the cost function. \n",
    "\n",
    "For this case we are going to use Adam Optimizer. In tensorflow we can use it by just single line of code. And then during training it will minimize the cost function that we have created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "opt = tf.train.AdamOptimizer().minimize(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set and ready to train our model now. But before training let's define some more variable like batch_size and epochs. Then we will define out tensorflow training session and then we will run it to train the model.\n",
    "\n",
    "Below is the implementation of what we have disccused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make Session\n",
    "net = tf.Session()\n",
    "\n",
    "# Run initializer\n",
    "net.run(tf.global_variables_initializer())\n",
    "\n",
    "# Number of epochs and batch size\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "mse_train = []\n",
    "mse_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start training. During the training session we are going to print the Train and Test error.By doing this we can keep a track of whether the cost is minimizing or not. Later on it can help us in debugging bais/variance problem as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train:  5.44015\n",
      "MSE Test:  6.5675\n",
      "Epoch 0, Batch 0\n",
      "MSE Train:  0.0012567\n",
      "MSE Test:  0.00706879\n",
      "Epoch 0, Batch 50\n",
      "MSE Train:  0.000495054\n",
      "MSE Test:  0.00800445\n",
      "Epoch 0, Batch 100\n",
      "MSE Train:  0.000368121\n",
      "MSE Test:  0.00828316\n",
      "Epoch 1, Batch 0\n",
      "MSE Train:  0.000258685\n",
      "MSE Test:  0.00809141\n",
      "Epoch 1, Batch 50\n",
      "MSE Train:  0.000551512\n",
      "MSE Test:  0.00475411\n",
      "Epoch 1, Batch 100\n",
      "MSE Train:  0.000183214\n",
      "MSE Test:  0.00626912\n",
      "Epoch 2, Batch 0\n",
      "MSE Train:  0.00103223\n",
      "MSE Test:  0.00360813\n",
      "Epoch 2, Batch 50\n",
      "MSE Train:  0.000142007\n",
      "MSE Test:  0.00417457\n",
      "Epoch 2, Batch 100\n",
      "MSE Train:  0.000392873\n",
      "MSE Test:  0.005805\n",
      "Epoch 3, Batch 0\n",
      "MSE Train:  0.000133606\n",
      "MSE Test:  0.00477723\n",
      "Epoch 3, Batch 50\n",
      "MSE Train:  0.000107574\n",
      "MSE Test:  0.00361394\n",
      "Epoch 3, Batch 100\n",
      "MSE Train:  0.0019377\n",
      "MSE Test:  0.00278047\n",
      "Epoch 4, Batch 0\n",
      "MSE Train:  8.42174e-05\n",
      "MSE Test:  0.00335123\n",
      "Epoch 4, Batch 50\n",
      "MSE Train:  8.56331e-05\n",
      "MSE Test:  0.00305501\n",
      "Epoch 4, Batch 100\n",
      "MSE Train:  0.000287475\n",
      "MSE Test:  0.00390383\n",
      "Epoch 5, Batch 0\n",
      "MSE Train:  0.000200337\n",
      "MSE Test:  0.00238841\n",
      "Epoch 5, Batch 50\n",
      "MSE Train:  0.000153078\n",
      "MSE Test:  0.00351452\n",
      "Epoch 5, Batch 100\n",
      "MSE Train:  0.000465244\n",
      "MSE Test:  0.00209917\n",
      "Epoch 6, Batch 0\n",
      "MSE Train:  0.000173638\n",
      "MSE Test:  0.00346379\n",
      "Epoch 6, Batch 50\n",
      "MSE Train:  6.81315e-05\n",
      "MSE Test:  0.00254172\n",
      "Epoch 6, Batch 100\n",
      "MSE Train:  6.19747e-05\n",
      "MSE Test:  0.0026719\n",
      "Epoch 7, Batch 0\n",
      "MSE Train:  8.94685e-05\n",
      "MSE Test:  0.00214307\n",
      "Epoch 7, Batch 50\n",
      "MSE Train:  6.25983e-05\n",
      "MSE Test:  0.0020525\n",
      "Epoch 7, Batch 100\n",
      "MSE Train:  6.63398e-05\n",
      "MSE Test:  0.00209787\n",
      "Epoch 8, Batch 0\n",
      "MSE Train:  0.000314125\n",
      "MSE Test:  0.001748\n",
      "Epoch 8, Batch 50\n",
      "MSE Train:  4.79922e-05\n",
      "MSE Test:  0.00198958\n",
      "Epoch 8, Batch 100\n",
      "MSE Train:  6.08499e-05\n",
      "MSE Test:  0.00203239\n",
      "Epoch 9, Batch 0\n",
      "MSE Train:  0.00032113\n",
      "MSE Test:  0.00164406\n",
      "Epoch 9, Batch 50\n",
      "MSE Train:  5.09744e-05\n",
      "MSE Test:  0.00180358\n",
      "Epoch 9, Batch 100\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "\n",
    "    # Shuffle training data\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "    X_train = X_train[shuffle_indices]\n",
    "    y_train = y_train[shuffle_indices]\n",
    "\n",
    "    # Minibatch training\n",
    "    for i in range(0, len(y_train) // batch_size):\n",
    "        start = i * batch_size\n",
    "        batch_x = X_train[start:start + batch_size]\n",
    "        batch_y = y_train[start:start + batch_size]\n",
    "        # Run optimizer with batch\n",
    "        net.run(opt, feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "        # Show progress\n",
    "        if np.mod(i, 50) == 0:\n",
    "            # MSE train and test\n",
    "            mse_train.append(net.run(mse, feed_dict={X: X_train, Y: y_train}))\n",
    "            mse_test.append(net.run(mse, feed_dict={X: X_test, Y: y_test}))\n",
    "            print('MSE Train: ', mse_train[-1])\n",
    "            print('MSE Test: ', mse_test[-1])\n",
    "            # Prediction\n",
    "            pred = net.run(out, feed_dict={X: X_test})\n",
    "            print('Epoch ' + str(e) + ', Batch ' + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the training you can see that the error for both taining data and testing data is reducing. \n",
    "\n",
    "So that's it now we have simple neural network which can be used for predictiong the stock market prices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
